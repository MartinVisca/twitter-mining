{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexi√≥n con la API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previo al desarrollo de la aplicaci√≥n, se debe pedir una autorizaci√≥n a la API de Twitter para poder acceder a los datos que se quieren analizar. Para √©sto se debe crear una app desde la secci√≥n Developers de Twitter, donde se consiguen los tokens necesarios para la autorizaci√≥n de acceso, los cuales se agregan al archivo \"twitter.ini\", usado en el c√≥digo a continuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se puede otorgar acceso a esta aplicaci√≥n con el siguiente link: \n",
      "https://api.twitter.com/oauth/authorize?oauth_token=CQ4jnQAAAAAA_VLpAAABbJD23Qs \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import configparser  \n",
    "import sys\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "def get_twitter_auth():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"twitter.ini\")\n",
    "    \n",
    "    try:\n",
    "        consumer_key = config.get(\"TwitterKeys\", \"ConsumerKey\")\n",
    "        consumer_secret = config.get(\"TwitterKeys\", \"ConsumerSecret\")\n",
    "        access_token = config.get(\"TwitterKeys\", \"AccessToken\")\n",
    "        access_secret = config.get(\"TwitterKeys\", \"AccessTokenSecret\")\n",
    "    except:\n",
    "        print(\"exception on %s!\" % option)\n",
    "        \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return auth\n",
    "\n",
    "def get_twitter_client():\n",
    "    auth = get_twitter_auth()\n",
    "    client = API(auth, wait_on_rate_limit = True)\n",
    "    return client\n",
    "\n",
    "auth = get_twitter_auth()\n",
    "client = API(auth)\n",
    "print(\"Se puede otorgar acceso a esta aplicaci√≥n con el siguiente link: \")\n",
    "print(auth.get_authorization_url(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenci√≥n de datos del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importo todas las librer√≠as necesarias para la obtenci√≥n de datos y creo la carpeta \"users\", donde se almacenar√°n los usuarios analizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from tweepy import Cursor\n",
    "\n",
    "MAX_FRIENDS = 1500\n",
    "\n",
    "user_name = \"delpotrojuan\"\n",
    "client = get_twitter_client()\n",
    "dir_name = \"users/{}\".format(user_name)\n",
    "\n",
    "max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
    "\n",
    "try:\n",
    "    os.makedirs(dir_name, mode = 0o755, exist_ok = True)\n",
    "except OSError:\n",
    "    print(\"El directorio {} ya existe\".format(dir_name))\n",
    "except Exception as e:\n",
    "    print(\"Error al crear el directorio {}\".format(dir_name))\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "def paginate(items, n):\n",
    "    for i in range(0, len(items), n):\n",
    "        yield items[i:i+n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenci√≥n de amigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El c√≥digo siguiente es el necesario para obtener los \"friends\" del usuario. De la misma manera se obtienen los seguidores, cambiando s√≥lo el \"folder_name\" (friends por followers). √âsto √∫ltimo no se plasm√≥ en este notebook por no ser necesario para el desarrollo del trabajo y por el hecho de tener b√°sicamente el mismo c√≥digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando 100 friends de un total de  23\n",
      "Descarga finalizada\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"users/{}/friends.jsonl\".format(user_name)\n",
    "\n",
    "with open(folder_name, 'w') as f:\n",
    "    for friends in Cursor(client.friends_ids, screen_name = user_name).pages(max_pages):\n",
    "        \n",
    "        for chunk in paginate(friends, 100):\n",
    "            \n",
    "            print(\"Descargando 100 friends de un total de \", len(friends))\n",
    "            users = client.lookup_users(user_ids = chunk)\n",
    "            \n",
    "            for user in users:\n",
    "                f.write(json.dumps(user._json)+\"\\n\")\n",
    "            \n",
    "            if len(friends) == 5000:\n",
    "                print(\"Espera de 60 segundos por el rate limit\")\n",
    "                time.sleep(60)\n",
    "                \n",
    "    print(\"Descarga finalizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfil de publicaciones del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User timeline del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como paso inicial, descargo una lista de los primeros tweets del timeline del usuario. En este caso seran 16 p√°ginas de 200 tweets cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando p√°gina 1 con 200 tweets\n",
      "Descargando p√°gina 2 con 200 tweets\n",
      "Descargando p√°gina 3 con 200 tweets\n",
      "Descargando p√°gina 4 con 200 tweets\n",
      "Descargando p√°gina 5 con 106 tweets\n",
      "Descarga finalizada\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"user_timeline_2019_{}.jsonl\".format(user_name)\n",
    "page_nro = 1\n",
    "\n",
    "with open(folder_name, 'w') as f:\n",
    "    \n",
    "    for page in Cursor(client.user_timeline, screen_name = user_name, count = 200).pages(16):\n",
    "        print(\"Descargando p√°gina {} con {} tweets\".format(page_nro, len(page)))    \n",
    "        page_nro += 1\n",
    "        \n",
    "        for status in page:\n",
    "            f.write(json.dumps(status._json) + \"\\n\")\n",
    "    \n",
    "    print ('Descarga finalizada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An√°lisis de los tweets descargados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se analizan los tweets del paso anterior con la librer√≠a \"NLTK\", para poder armar un perfil de intereses en base a ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importo las herramientas de an√°lisis necesarias y defino el m√©todo \"process\" para tokenizar los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process(text, stopwords = []):\n",
    "    tokenizer = TweetTokenizer(strip_handles = True, reduce_len = True)\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengo la lista de \"stopwords\". La misma estar√° compuesta por las propias de los idiomas espa√±ol e ingl√©s (para darle amplitud al an√°lisis) m√°s las surgidas en las distintas ejecuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('spanish') + punct + ['rt', 'via', '...', '‚Ä¶', 'si', '!', '¬°', 'jajaja', 'jajajaja', 'jajajajaja', '@', 'mas', 'voy', 'vos', 'sos', 'soy', 'üá∑', '\"', ' ', '..', '¬ø', '?', 'ser', 'pessoas', 'seu', 'perfil', '‚Äú', 'q', '‚Äô', '‚Äù'] + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consigo los tokens a partir de los tweets y las stopwords; como dije anteriormente, esta lista se fue modificando en las distintas ejecuciones, al surgir palabras vac√≠as para el an√°lisis. Luego se imprimen las 20 m√°s comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gracias: 146\n",
      "feliz: 56\n",
      "hola: 55\n",
      "saludos: 54\n",
      "gran: 53\n",
      "entrenamiento: 49\n",
      "great: 48\n",
      "amigos: 47\n",
      "d√≠a: 47\n",
      "üòÄ: 47\n",
      "dejo: 46\n",
      "Ô∏è: 45\n",
      "mensajes: 42\n",
      "hoy: 41\n",
      "üéæ: 40\n",
      "happy: 38\n",
      "üí™: 36\n",
      "apoyo: 36\n",
      "semana: 36\n",
      "video: 36\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"user_timeline_2019_{}.jsonl\".format(user_name)\n",
    "tf = Counter()\n",
    "\n",
    "with open(folder_name, 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text = tweet.get('text', ''), stopwords = stopword_list)\n",
    "        tf.update(tokens)\n",
    "        \n",
    "    for tag, count in tf.most_common(20):\n",
    "        print(\"{}: {}\".format(tag, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos pueden visualizarse de manera gr√°fica, como se ver√° en la ejecuci√≥n siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folder_name = \"user_timeline_2019_{}.jsonl\".format(user_name)\n",
    "tf = Counter()\n",
    "with open(folder_name, 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text = tweet.get('text', ''), stopwords = stopword_list)\n",
    "        tf.update(tokens)\n",
    "        \n",
    "    y = [count for tag, count in tf.most_common(1000)]\n",
    "    x = range(1, len(y) + 1)\n",
    "    plt.bar(x, y)\n",
    "    plt.title(\"T√©rmino o token\")\n",
    "    plt.ylabel(\"Cantidad de ocurrencias\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de estos datos conseguidos se pueden analizar palabras clave para lograr encontrar un perfil del usuario. Por ejemplo, si entre las palabras clave se observan \"f√∫tbol\", \"partido\", nombres de clubes, nombres de jugadores, etc., puedo decir que el usuario tiene un perfil orientado al deporte, espec√≠ficamente al f√∫tbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfil de intereses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las formas de definir un perfil de intereses de un usuario es viendo qu√© cuentas sigue; un usuario que siga muchas cuentas relacionadas con deportes conducir√° a la deducci√≥n de un inter√©s relacionado a ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tengo que analizar todos los perfiles que sigue el usuario (los \"friends\"), lo mas conveniente es convertir el c√≥digo de obtenci√≥n de tweets y an√°lisis de los mismos en funciones que facilitar√°n su uso en cada perfil seguido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_usuario(user):\n",
    "    folder_name = \"user_timeline_2019_{}.jsonl\".format(user)\n",
    "\n",
    "    with open(folder_name, 'w') as f:\n",
    "    \n",
    "        for page in Cursor(client.user_timeline, screen_name = user, count = 100).pages(4):  #Un menor numero de tweets y paginas para reducir los tiempos de ejecucion\n",
    "        \n",
    "            for status in page:\n",
    "                f.write(json.dumps(status._json) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_usuario(user, tf):\n",
    "    folder_name = \"user_timeline_2019_{}.jsonl\".format(user)\n",
    "\n",
    "    with open(folder_name, 'r') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tokens = process(text = tweet.get('text', ''), stopwords = stopword_list)\n",
    "            tf.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tengo las funciones definidas, s√≥lo me queda aplicarlas. En el c√≥digo a continuaci√≥n se consiguen los 20 tokens mas comunes presentes en todos los usuarios seguidos.\n",
    "\n",
    "En un recorrido por todos los \"friends\" del usuario, se van obteniendo los tweets de cada uno y analiz√°ndolos a la vez. Este an√°lisis se ver√° reflejado en el resultado final de la variable \"tf\", la cual almacena los tokens y la cantidad de ocurrencias de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando\n",
      "Descarga finalizada\n",
      "\n",
      "Ô∏è: 1031\n",
      "#wimbledon: 459\n",
      "üëè: 398\n",
      "üèº: 385\n",
      "üí™: 354\n",
      "üéæ: 318\n",
      "üèÜ: 308\n",
      "üèª: 305\n",
      "#rg19: 282\n",
      "‚Äç: 272\n",
      "final: 252\n",
      "üá¶: 237\n",
      "first: 236\n",
      "one: 232\n",
      "hoy: 225\n",
      "day: 224\n",
      "üî•: 218\n",
      "‚ù§: 218\n",
      "üôå: 213\n",
      "üôè: 198\n"
     ]
    }
   ],
   "source": [
    "tf = Counter()\n",
    "client = get_twitter_client()\n",
    "folder_name = \"users/{}/friends.jsonl\".format(user_name)\n",
    "\n",
    "with open(folder_name, 'r') as archivo:\n",
    "    print ('Descargando')\n",
    "    \n",
    "    for user_name in archivo:\n",
    "        try:\n",
    "            profile = json.loads(user_name)\n",
    "            user = profile['screen_name']\n",
    "            get_tweets_usuario(user)\n",
    "            analizar_usuario(user, tf)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    print ('Descarga finalizada')\n",
    "    print ('')\n",
    "        \n",
    "for tag, count in tf.most_common(20):\n",
    "    print(\"{}: {}\".format(tag, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos serian los tokens mas comunes obtenidos de las cuentas que sigue el usuario. A partir de ellos se obtiene un \"perfil de intereses\" que describir√° la din√°mica del usuario en relaci√≥n a su propio perfil y los seguidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta informaci√≥n tambi√©n podr√≠a ser graficada, al igual que se hizo anteriormente para los tokens de un usuario particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de determinar los intereses de un usuario es agrupando sus tweets por palabras claves. Este algoritmo se denomina clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, el c√≥digo donde se realiza el clustering del usuario analizado. El n√∫mero de clusters ser√° variable hasta encontrar el valor que mejor se ajuste al an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos: (3199, 200)\n",
      "---------------- CLUSTER 4 ----------------\n",
      "\n",
      "-> Este agosto nos unimos para defender lo importante. https://t.co/DSEN6vBnw0\n",
      "-> https://t.co/ODrwwGhjNQ\n",
      "-> Por lo que le pido disculpas si se sinti√≥ ofendido, porque estuvo muy lejos mi intenci√≥n de hacerle ning√∫n tipo de‚Ä¶ https://t.co/UGLM0kKEOp\n",
      "-> Acabo de leer el twitt de Luis Novaresio en el que considera que le hice una imputaci√≥n de cuasi torturador al refe‚Ä¶ https://t.co/ormp0Yuf0G\n",
      "-> Axel en una palabra: Gobernador. https://t.co/X1xzgfM1nc\n",
      "-> Es necesario que todos hagamos el esfuerzo y tengamos la responsabilidad de organizarnos para que esto cambie. Esto‚Ä¶ https://t.co/cbE04ClJgk\n",
      "-> Humor y esperanza. Me encanta üòç https://t.co/hNk62NmXni\n",
      "\n",
      "\n",
      "---------------- CLUSTER 1 ----------------\n",
      "\n",
      "-> Tenemos que convocar a todos los argentinos y a todas las argentinas a una unidad nacional para que la gente pueda‚Ä¶ https://t.co/CtlzfF7QFo\n",
      "-> ‚ÄúMinisterio de la Sinceridad - Prescindencia de la Naci√≥n‚Äù \n",
      "\n",
      "Alguien en la red me env√≠a este video, quiero comparti‚Ä¶ https://t.co/e1kSJtKd9y\n",
      "-> los que por ah√≠ privilegiamos nuestras discusiones, nuestras peleas, bueno, ya nos dimos cuenta que cuando nos pele‚Ä¶ https://t.co/AIpii7zUeh\n",
      "-> As√≠ que con mucha fe y mucha esperanza, pese a las dificultades, estoy convencida de que con los compa√±eros y las c‚Ä¶ https://t.co/NILEYT7ekF\n",
      "-> Tenemos que convocar a todos los argentinos y a las argentinas a una tarea que sabemos que no va a ser f√°cil, con l‚Ä¶ https://t.co/IxoGRpAQXS\n",
      "-> Esta deuda monumental que han contra√≠do, m√°s que un Plan Marshall, m√°s que la deuda que se contrajo durante la dict‚Ä¶ https://t.co/qrWMnAiWZD\n",
      "-> Y creo entonces que todos y todas vamos a tener que poner un gran esfuerzo para superar estos intentos tan terrible‚Ä¶ https://t.co/Uxcs4ZRkW5\n",
      "\n",
      "\n",
      "---------------- CLUSTER 3 ----------------\n",
      "\n",
      "-> Es tan obsceno el blindaje medi√°tico que tienen Macri y Vidal que, como en otras tristes √©pocas, para entender lo q‚Ä¶ https://t.co/kiKQelMOSh\n",
      "-> La citaci√≥n de hoy se trata de una denuncia efectuada por el gobierno de Mauricio Macri sobre obras p√∫blicas viales‚Ä¶ https://t.co/a1qz9D9XVo\n",
      "-> Al presidente Mauricio Macri y su familia, mis condolencias por el fallecimiento de su padre.\n",
      "-> Ayer en Jujuy, un paso m√°s en la brutal persecuci√≥n que sufre Milagro Sala, con una condena a medida de Macri y Ger‚Ä¶ https://t.co/O5621TiZLN\n",
      "-> Digo yo: ¬øD√≥nde se escribir√°n las sentencias? Todo a pedido y a medida de Macri, Cambiemos, Clar√≠n y sobre todo: el‚Ä¶ https://t.co/I4fNzHgF7L\n",
      "-> Por si no te acord√°s, Irurzun es el de la foto con el principal operador judicial de Macri y Cambiemos: Pep√≠n Rodr√≠‚Ä¶ https://t.co/gE2E0TJC92\n",
      "-> Macri dice que sin Cambiemos en el gobierno esta tormenta hubiera terminado como en el 2001; y un ‚Äúarrepentido‚Äù del‚Ä¶ https://t.co/wX4sN7Re8e\n",
      "\n",
      "\n",
      "---------------- CLUSTER 5 ----------------\n",
      "\n",
      "-> La campa√±a sucia, y muy violenta, por parte del gobierno me tiene muy preocupada.\n",
      "¬øQu√© fue lo que pas√≥ en estos tre‚Ä¶ https://t.co/Qp1ptiNFr0\n",
      "-> La campa√±a sucia y muy violenta por parte del gobierno me tiene preocupada. Me tiene preocupada porque las campa√±as‚Ä¶ https://t.co/e3ZyFuwmkf\n",
      "-> Argentina merece otro gobierno.\n",
      "\n",
      "Ya es tiempo de TOD‚òÄÔ∏èS. https://t.co/oinnKCEj6F\n",
      "-> Me contaba en detalle lo que cada uno de ellos hab√≠a dicho sobre √©l y nuestro gobierno por la firma del acuerdo. No‚Ä¶ https://t.co/KdNA0BEBx9\n",
      "-> Cada vez es m√°s evidente que las medidas econ√≥micas del gobierno de Cambiemos, que tanto sufrimiento provocan a los‚Ä¶ https://t.co/KLbQrLVYst\n",
      "-> Este ya ni siquiera es un gobierno de ricos para ricos, sino de algunos ricos para ellos mismos y para sus amigos.\n",
      "-> ¬øTe parece que es s√≥lo una palabra y que estamos exagerando? Mir√° lo qu√© pasa mientras tanto.\n",
      "\n",
      "El Gobierno habilita‚Ä¶ https://t.co/kVmimS9cTT\n",
      "\n",
      "\n",
      "---------------- CLUSTER 2 ----------------\n",
      "\n",
      "-> RT @UniCiudadanaAR: Vivir con la garganta y el est√≥mago estrujados porque no sabemos qu√© es lo que nos va a pasar... eso no es una buena vi‚Ä¶\n",
      "-> RT @UniCiudadanaAR: Hay esperanza. Hay futuro.\n",
      "#EsConTodos #EsConTodas https://t.co/iwIZ3u6Dz7\n",
      "-> RT @UniCiudadanaAR: .@CFKArgentina: \"Es necesario un contrato social de los argentinos y las argentinas\".\n",
      "\n",
      "#Sinceramente https://t.co/jOCns‚Ä¶\n",
      "-> RT @UniCiudadanaAR: Cristina recibi√≥ hoy en el @inst_PATRIAar a un grupo de artistas pl√°sticos argentinos. https://t.co/WFOgMP3Lm9\n",
      "-> RT @UniCiudadanaAR: ‚ÄúSe oper√≥ sobre la sociedad para romper los v√≠nculos de solidaridad. As√≠ oper√≥ el neoliberalismo.‚Äù\n",
      "\n",
      "#CristinaEnClacso #‚Ä¶\n",
      "-> RT @UniCiudadanaAR: M√°ximo Kirchner estuvo en #ElDestapeRadio \n",
      "\n",
      "Volv√© a ver y escuchar la entrevista con @robdnavarro \n",
      "üëâüèΩ https://t.co/ZJjU‚Ä¶\n",
      "-> RT @UniCiudadanaAR: En un mes el pa√≠s ya perdi√≥ un tercio de los d√≥lares que ingresaron por el acuerdo con el FMI.\n",
      "\n",
      "¬øD√≥nde va la plata de l‚Ä¶\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "user_name = \"CFKArgentina\"\n",
    "\n",
    "connections_file = \"user_timeline_2019_{}.jsonl\".format(user_name)\n",
    "\n",
    "with open(connections_file) as f:\n",
    "    tweets = []\n",
    "    \n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(tweet['text'])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df = 0.8, min_df = 2, max_features = 200, stop_words = stopword_list,  ngram_range = (1, 3), \n",
    "                                 use_idf = True)\n",
    "    X = vectorizer.fit_transform(tweets)\n",
    "    print(\"Dimensiones de los datos: {}\".format(X.shape))\n",
    "    \n",
    "    km = KMeans(n_clusters = 5)\n",
    "    km.fit(X)\n",
    "    clusters = defaultdict(list)\n",
    "    \n",
    "    for i, label in enumerate(km.labels_):\n",
    "        clusters[label].append(tweets[i])\n",
    "        \n",
    "    for label, descriptions in clusters.items():\n",
    "        print('---------------- CLUSTER {} ----------------'.format(label + 1))\n",
    "        print ('')\n",
    "        for desc in descriptions[:7]:\n",
    "            print('-> ' + desc)\n",
    "        print ('')\n",
    "        print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocalizaci√≥n de tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una opci√≥n en los tweets de activar la ubicaci√≥n desde la cual se efectuaron. Esta informaci√≥n de localizaci√≥n puede analizarse; a continuaci√≥n se ve como crear un archivo (de extensi√≥n json) en el cual se guardaran las porciones de los tweets necesarias para el an√°lisis, es decir, todo lo referido a la ubicaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweets_traffic.jsonl\", 'r', encoding = 'utf-8-sig') as f:\n",
    "    geo_data = {\n",
    "        \"type\": \"FeatureCollection\", \"features\": []\n",
    "    }\n",
    "    \n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        try:\n",
    "            if tweet['geolocation']:\n",
    "                geo_json_feature = {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"Point\",\n",
    "                        \"coordinates\": tweet['geolocation']\n",
    "                    },\n",
    "                    \"properties\": {\n",
    "                        \"text\": tweet['text'],\n",
    "                        \"created_at\": tweet['creation_time']\n",
    "                    }\n",
    "                }\n",
    "                geo_data['features'].append(geo_json_feature)\n",
    "                \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    with open(\"tweets_map_test.geo.json\", 'w') as fout:\n",
    "        fout.write(json.dumps(geo_data, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este archivo json creado puede, a su vez, ser analizado. Y para este an√°lisis existe la opci√≥n de visualizarlo en mapas. A continuaci√≥n se crear√° un mapa con la informacion obtenida, en tres pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, marcamos individualmente cada uno de los tweets en el mapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium \n",
    "\n",
    "tweet_map = folium.Map(location = [-37.320480, -59.132904], zoom_start = 4)\n",
    "geodata = json.load(open(\"tweets_map_test.geo.json\", encoding = \"utf-8-sig\"))\n",
    "\n",
    "for tweet in geodata['features']:\n",
    "    marker = folium.Marker([tweet['geometry']['coordinates']['latitude'], tweet['geometry']['coordinates']['longitude']], \n",
    "                           popup = tweet['properties']['text'])\n",
    "    marker.add_to(tweet_map)\n",
    "\n",
    "tweet_map.save(\"mapa_de_tweets.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego agrupamos estos marcadores en clusters, los cuales incluir√°n m√°s o menos datos seg√∫n el zoom con el que se est√© visualizando el mapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "tweet_map = folium.Map(location = [-37.320480, -59.132904], zoom_start = 4)\n",
    "marker_cluster = MarkerCluster().add_to(tweet_map)\n",
    "\n",
    "for tweet in geodata['features']:\n",
    "    marker = folium.Marker([tweet['geometry']['coordinates']['latitude'], tweet['geometry']['coordinates']['longitude']], \n",
    "                           popup = tweet['properties']['text'])\n",
    "    marker.add_to(marker_cluster)\n",
    "\n",
    "tweet_map.save(\"mapa_de_tweets.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo, agregamos a cada marcador el texto del tweet; con esta funci√≥n se podr√° ver cual es el tweet relacionado al marcador con s√≥lo clickear sobre √©l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_map = folium.Map(location = [-37.320480, -59.132904], zoom_start = 4)\n",
    "marker_cluster = MarkerCluster().add_to(tweet_map)\n",
    "\n",
    "for tweet in geodata['features']:\n",
    "    marker = folium.Marker([tweet['geometry']['coordinates']['latitude'], tweet['geometry']['coordinates']['longitude']], \n",
    "                           popup = tweet['properties']['text'])\n",
    "    marker.add_to(marker_cluster)\n",
    "    \n",
    "tweet_map.save(\"mapa_de_tweets.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
